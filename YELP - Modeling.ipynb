{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac12ee2",
   "metadata": {},
   "source": [
    "# Yelp Reviews Sentiment Analysis - Modeling\n",
    "\n",
    "#### Prepared By: Rabia Tariq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c90439a",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* [Introduction](#Introduction)\n",
    "* [Imports](#Imports)\n",
    "* [Building Models Using Features we Extracted](#Features)\n",
    "    * [Logistic Regression](#Logreg)\n",
    "    * [Random Forest](#RF)\n",
    "    * [Gradient Boosting Classifier](#GBC)\n",
    "    * [Support Vector Classifier](#SVC)\n",
    "    * [Result](#Result)\n",
    "* [Word Embedding: Bag of Words](#BOW)\n",
    "    * [Naive Bayes with TF-IDF](#Nb_tfidf)\n",
    "    * [Naive Bayes with CountVectorizer](#Nb_cv)\n",
    "    * [Gradient Boosting Classifier with TF-IDF](#GBC_tfidf)\n",
    "    * [Result](#Result)\n",
    "* [Ensembles](#Ensemble)\n",
    "    * [Sparse and Dense Features](#Sparse)\n",
    "    * [Naive Bayes Probability](#Nb_proba)\n",
    "    * [Stacked Model (Gradient Boosting + Naive Bayes)](#Stacked)\n",
    "    * [Result](#Result)\n",
    "* [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d35d65",
   "metadata": {},
   "source": [
    "## Introduction<a id='Introduction'></a>\n",
    "\n",
    "Up until now, we have done data wrangling, performed EDA, done preprocessing and found the most important feautres. We created a baseline model and got an accuracy of ~ 68% by using Logistic Regression. In this notebook, we will be creating many different models and find the most accurate one which we can use in the future.\n",
    "\n",
    "We will be using Ensembling techniques, Bag of words model and stack different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d440d8b",
   "metadata": {},
   "source": [
    "## Imports<a id='Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d9ecca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from pycm import *\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5532e1f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Review</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>%_Positive_Words</th>\n",
       "      <th>park</th>\n",
       "      <th>bake</th>\n",
       "      <th>shop</th>\n",
       "      <th>becom</th>\n",
       "      <th>go</th>\n",
       "      <th>...</th>\n",
       "      <th>sangria</th>\n",
       "      <th>time squar</th>\n",
       "      <th>jukebox</th>\n",
       "      <th>calzon</th>\n",
       "      <th>byob</th>\n",
       "      <th>pizzeria</th>\n",
       "      <th>bake clam</th>\n",
       "      <th>castl</th>\n",
       "      <th>white castl</th>\n",
       "      <th>drive thru</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Morris Park Bake Shop</td>\n",
       "      <td>'Morris Park Bake Shop has become my go to spo...</td>\n",
       "      <td>0.338889</td>\n",
       "      <td>Somewhat Positive</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Morris Park Bake Shop</td>\n",
       "      <td>'I thought the cookies and biscotti were prett...</td>\n",
       "      <td>0.314583</td>\n",
       "      <td>Somewhat Positive</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Morris Park Bake Shop</td>\n",
       "      <td>'Guys.... so Im a big time biscotti connoisseu...</td>\n",
       "      <td>0.238068</td>\n",
       "      <td>Somewhat Positive</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Morris Park Bake Shop</td>\n",
       "      <td>'I had a craving for a special type of cake wi...</td>\n",
       "      <td>0.314643</td>\n",
       "      <td>Somewhat Positive</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Morris Park Bake Shop</td>\n",
       "      <td>'The chocolate cups are amazing! Have been eat...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1587 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name                                             Review  \\\n",
       "0  Morris Park Bake Shop  'Morris Park Bake Shop has become my go to spo...   \n",
       "1  Morris Park Bake Shop  'I thought the cookies and biscotti were prett...   \n",
       "2  Morris Park Bake Shop  'Guys.... so Im a big time biscotti connoisseu...   \n",
       "3  Morris Park Bake Shop  'I had a craving for a special type of cake wi...   \n",
       "4  Morris Park Bake Shop  'The chocolate cups are amazing! Have been eat...   \n",
       "\n",
       "   Polarity          Sentiment  %_Positive_Words  park  bake  shop  becom  \\\n",
       "0  0.338889  Somewhat Positive          0.206897   0.0   0.0   0.0    0.0   \n",
       "1  0.314583  Somewhat Positive          0.130435   0.0   0.0   0.0    0.0   \n",
       "2  0.238068  Somewhat Positive          0.127660   0.0   0.0   0.0    0.0   \n",
       "3  0.314643  Somewhat Positive          0.218750   0.0   0.0   0.0    0.0   \n",
       "4  0.500000           Positive          0.222222   0.0   0.0   0.0    0.0   \n",
       "\n",
       "    go  ...  sangria  time squar  jukebox  calzon  byob  pizzeria  bake clam  \\\n",
       "0  0.0  ...      0.0         0.0      0.0     0.0   0.0       0.0        0.0   \n",
       "1  0.0  ...      0.0         0.0      0.0     0.0   0.0       0.0        0.0   \n",
       "2  0.0  ...      0.0         0.0      0.0     0.0   0.0       0.0        0.0   \n",
       "3  0.0  ...      0.0         0.0      0.0     0.0   0.0       0.0        0.0   \n",
       "4  0.0  ...      0.0         0.0      0.0     0.0   0.0       0.0        0.0   \n",
       "\n",
       "     castl  white castl  drive thru  \n",
       "0  0.00000          0.0         0.0  \n",
       "1  0.00000          0.0         0.0  \n",
       "2  0.15229          0.0         0.0  \n",
       "3  0.00000          0.0         0.0  \n",
       "4  0.00000          0.0         0.0  \n",
       "\n",
       "[5 rows x 1587 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_data = pd.read_csv('yelp_data_preprocessing.csv')\n",
    "yelp_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ed8ad2",
   "metadata": {},
   "source": [
    "## Building models using the features we extracted<a id='Features'></a>\n",
    "\n",
    "We will start by creating a model using Logistic Regression and Random Forest. Perform hyperparameter tuning to get the best performance from our models\n",
    "\n",
    "We will be using F1 scores and accuracy to see the performance of our models.\n",
    "\n",
    "In our modeling process, we will be:\n",
    "\n",
    "- Defining a pipeline and use GridSearch to find the best parameters for our chosen model type\n",
    "- Measuring the accuracy and F1 scores\n",
    "- Repeat the same steps for our next model of choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca36d4ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = yelp_data.iloc[0:,4:]\n",
    "y = yelp_data.Sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10546318",
   "metadata": {},
   "source": [
    "### Logistic Regression<a id='Logreg'></a>\n",
    "\n",
    "Logistic regression is basically a supervised classification algorithm. In a classification problem, the target variable(or output), y, can take only discrete values for given set of features(or inputs), X.\n",
    "\n",
    "We will be creating a pipleline that uses Standard Scaler and Logistic Regression and then use GridSearchCV to find the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c625ac78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr__C': 0.1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = [('scaler', StandardScaler()), ('lr', LogisticRegression(solver = 'lbfgs'))] \n",
    "pipeline = Pipeline(pipe)\n",
    "params = {'lr__C':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "l_reg = GridSearchCV(pipeline, params, cv = 10, scoring = \"accuracy\") \n",
    "l_reg.fit(X_train, y_train)\n",
    "l_reg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959a3e03",
   "metadata": {},
   "source": [
    "Now we will be using pickle to save our models. Pickle is very useful for when you're working with machine learning algorithms, where you want to save them to be able to make new predictions at a later time, without having to rewrite everything or train the model all over again.\n",
    "\n",
    "https://www.datacamp.com/community/tutorials/pickle-python-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5169aa58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'l_reg.sav'\n",
    "pickle.dump(l_reg, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36d62caa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'l_reg.sav'\n",
    "l_reg = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65d9a2d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = l_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b82ccda1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.6896375701888718\n",
      "F1 Score (macro):  0.6898171496267322\n",
      "F1 Score (micro):  0.6896375701888718\n",
      "F1 Score (weighted):  0.6892833011179479\n"
     ]
    }
   ],
   "source": [
    "test_acc = l_reg.score(X_test, y_test)\n",
    "f1_acc = f1_score(y_test, result, average = 'macro')\n",
    "f1_acc_mic = f1_score(y_test, result, average = 'micro')\n",
    "f1_acc_w = f1_score(y_test, result, average = 'weighted')\n",
    "print(\"Accuracy on test data: \" ,test_acc)\n",
    "print('F1 Score (macro): ', f1_acc)\n",
    "print('F1 Score (micro): ', f1_acc_mic)\n",
    "print('F1 Score (weighted): ', f1_acc_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e0fd60e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lreg_acc = test_acc\n",
    "lreg_f1_mac = f1_acc\n",
    "lreg_f1_mic = f1_acc_mic\n",
    "lreg_f1_w = f1_acc_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d04689",
   "metadata": {},
   "source": [
    "### Random Forest Classifier<a id='RF'></a>\n",
    "\n",
    "\n",
    "A Random Forest is an ensemble technique capable of performing both regression and classification tasks with the use of multiple decision trees and a technique called Bootstrap and Aggregation, commonly known as bagging. The basic idea behind this is to combine multiple decision trees in determining the final output rather than relying on individual decision trees. Predictions are made by averaging the predictions of each decision tree. Or, to extend the analogy much like a forest is a collection of trees, the random forest model is also a collection of decision tree models. Random Forest has multiple decision trees as base learning models. We randomly perform row sampling and feature sampling from the dataset forming sample datasets for every model. This part is called Bootstrap. This makes random forests a strong modeling technique that’s much more powerful than a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9bc5dd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__max_features': 'auto', 'rf__n_estimators': 50}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = [('scaler', StandardScaler()), ('rf', RandomForestClassifier())] \n",
    "pipeline = Pipeline(pipe)\n",
    "params = {'rf__n_estimators': [10 , 20, 30, 40, 50], 'rf__max_features': ['auto','sqrt']}\n",
    "\n",
    "rf = GridSearchCV(pipeline, params, cv = 10, scoring = \"accuracy\") \n",
    "rf.fit(X_train, y_train)\n",
    "rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adbb208f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'rf.sav'\n",
    "pickle.dump(rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dc80b93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'rf.sav'\n",
    "rf = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e525e17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e99e7fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.6391015824400205\n",
      "F1 Score (macro):  0.6470514274085106\n",
      "F1 Score (micro):  0.6391015824400205\n",
      "F1 Score (weighted):  0.6387210131962797\n"
     ]
    }
   ],
   "source": [
    "test_acc = rf.score(X_test, y_test)\n",
    "f1_acc_mac = f1_score(y_test, result, average = 'macro')\n",
    "f1_acc_mic = f1_score(y_test, result, average = 'micro')\n",
    "f1_acc_w = f1_score(y_test, result, average = 'weighted')\n",
    "print(\"Accuracy on test data: \" ,test_acc)\n",
    "print('F1 Score (macro): ', f1_acc_mac)\n",
    "print('F1 Score (micro): ', f1_acc_mic)\n",
    "print('F1 Score (weighted): ', f1_acc_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51d03cfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_acc = test_acc\n",
    "rf_f1_mac = f1_acc_mac\n",
    "rf_f1_mic = f1_acc_mic\n",
    "rf_f1_w = f1_acc_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b65ab12",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier<a id='GBC'></a>\n",
    "\n",
    "Same thing here, first we'll make a pipeline and then use gridsearch to find the best parameters to tune this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd31af56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gbc__learning_rate': 0.15, 'gbc__n_estimators': 500}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = [('scaler', StandardScaler()), ('gbc', GradientBoostingClassifier(max_features='sqrt'))] \n",
    "pipeline = Pipeline(pipe)\n",
    "params = {'gbc__n_estimators':[10, 50, 100, 200, 500], 'gbc__learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25]}\n",
    "\n",
    "gbc = GridSearchCV(pipeline, params, cv = 10, scoring = \"accuracy\") \n",
    "gbc.fit(X_train, y_train)\n",
    "gbc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6ca7ef8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'gbc.sav'\n",
    "pickle.dump(gbc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc9e1b0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'gbc.sav'\n",
    "gbc = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7405fc6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = gbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9139539b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.6906584992343032\n",
      "F1 Score (macro):  0.6949487692162887\n",
      "F1 Score (micro):  0.6906584992343032\n",
      "F1 Score (weighted):  0.6904596183071956\n"
     ]
    }
   ],
   "source": [
    "test_acc = gbc.score(X_test, y_test)\n",
    "f1_acc_mac = f1_score(y_test, result, average = 'macro')\n",
    "f1_acc_mic = f1_score(y_test, result, average = 'micro')\n",
    "f1_acc_w = f1_score(y_test, result, average = 'weighted')\n",
    "print(\"Accuracy on test data: \" ,test_acc)\n",
    "print('F1 Score (macro): ', f1_acc_mac)\n",
    "print('F1 Score (micro): ', f1_acc_mic)\n",
    "print('F1 Score (weighted): ', f1_acc_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5ae469e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbc_acc = test_acc\n",
    "gbc_f1_mac = f1_acc_mac\n",
    "gbc_f1_mic = f1_acc_mic\n",
    "gbc_f1_w = f1_acc_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5a7d2",
   "metadata": {},
   "source": [
    "### Support Vector Classification<a id='SVC'></a>\n",
    "\n",
    "Same thing here, first we'll make a pipeline and then use gridsearch to find the best parameters to tune this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92fd0992",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svc__C': 0.01}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = [('scaler', StandardScaler()), ('svc', SVC(probability=False,kernel='linear',gamma='auto'))] \n",
    "pipeline = Pipeline(pipe)\n",
    "params = {'svc__C':[0.01, 0.1, 1]}\n",
    "\n",
    "svc = GridSearchCV(pipeline, params, cv = 10, scoring = \"accuracy\") \n",
    "svc.fit(X_train, y_train)\n",
    "svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60f15621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'svc.sav'\n",
    "pickle.dump(svc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03bf073d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'svc.sav'\n",
    "svc = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eac51f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5db904e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.6932108218478815\n",
      "F1 Score (macro):  0.6929348533871663\n",
      "F1 Score (micro):  0.6932108218478815\n",
      "F1 Score (weighted):  0.6922096489767159\n"
     ]
    }
   ],
   "source": [
    "test_acc = svc.score(X_test, y_test)\n",
    "f1_acc_mac = f1_score(y_test, result, average = 'macro')\n",
    "f1_acc_mic = f1_score(y_test, result, average = 'micro')\n",
    "f1_acc_w = f1_score(y_test, result, average = 'weighted')\n",
    "print(\"Accuracy on test data: \" ,test_acc)\n",
    "print('F1 Score (macro): ', f1_acc_mac)\n",
    "print('F1 Score (micro): ', f1_acc_mic)\n",
    "print('F1 Score (weighted): ', f1_acc_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae8b1990",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svc_acc = test_acc\n",
    "svc_f1_mac = f1_acc_mac\n",
    "svc_f1_mic = f1_acc_mic\n",
    "svc_f1_w = f1_acc_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4513d44",
   "metadata": {},
   "source": [
    "### Result<a id='Result'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13dee947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Micro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting Classifier</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model  Accuracy  F1_Macro  F1_Micro  F1_Weighted\n",
       "0           Logistic Regression     0.690     0.690     0.690        0.689\n",
       "1                 Random Forest     0.639     0.647     0.639        0.639\n",
       "2  Gradient Boosting Classifier     0.691     0.695     0.691        0.690\n",
       "3                           SVC     0.693     0.693     0.693        0.692"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'Model':['Logistic Regression', 'Random Forest', 'Gradient Boosting Classifier',  'SVC'],\n",
    "             'Accuracy':[lreg_acc, rf_acc, gbc_acc, svc_acc],\n",
    "             'F1_Macro':[lreg_f1_mac, rf_f1_mac, gbc_f1_mac, svc_f1_mac],\n",
    "             'F1_Micro':[lreg_f1_mic, rf_f1_mic, gbc_f1_mic, svc_f1_mic],\n",
    "             'F1_Weighted':[lreg_f1_w, rf_f1_w, gbc_f1_w, svc_f1_w]})\n",
    "\n",
    "df = df.round(3)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d1401",
   "metadata": {},
   "source": [
    "## Word Embedding: Bag of Words<a id='BOW'></a>\n",
    "\n",
    "Bag of words is a Natural Language Processing technique of text modelling. In technical terms, we can say that it is a method of feature extraction with text data. A bag of words is a representation of text that describes the occurrence of words within a document. We just keep track of word counts and disregard the grammatical details and the word order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "309e5edb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = yelp_data.Review\n",
    "y = yelp_data.Sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa69d13b",
   "metadata": {},
   "source": [
    "### Naive Bayes with TF-IDF<a id='Nb_tfidf'></a>\n",
    "\n",
    "Tf-idf short for “term frequency-inverse document frequency”, which basically reflects how important a word is to a document (email) in a collection or corpus (our set of emails or documents).\n",
    "The tf-idf is an statistic that increases with the number of times a word appears in the document, penalized by the number of documents in the corpus that contain the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8285cec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 0.01, 'vec__min_df': 0.01}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = [('vec', TfidfVectorizer(stop_words = 'english', ngram_range = (1, 2))), ('nb', MultinomialNB())] \n",
    "pipeline = Pipeline(pipe)\n",
    "params =  {'vec__min_df':[0.01, 0.1, 1, 10, 100], 'nb__alpha':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "nb = GridSearchCV(pipeline, params, cv = 10, scoring = \"accuracy\") \n",
    "nb.fit(X_train, y_train)\n",
    "nb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c723f57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'nb_tfidf.sav'\n",
    "pickle.dump(nb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6997292e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'nb_tfidf.sav'\n",
    "nb = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "435d8413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca56f0b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.5814190913731496\n",
      "F1 Score (macro):  0.5624028308309963\n",
      "F1 Score (micro):  0.5814190913731496\n",
      "F1 Score (weighted):  0.5682764704916207\n"
     ]
    }
   ],
   "source": [
    "test_acc = nb.score(X_test, y_test)\n",
    "f1_acc_mac = f1_score(y_test, result, average = 'macro')\n",
    "f1_acc_mic = f1_score(y_test, result, average = 'micro')\n",
    "f1_acc_w = f1_score(y_test, result, average = 'weighted')\n",
    "print(\"Accuracy on test data: \" ,test_acc)\n",
    "print('F1 Score (macro): ', f1_acc_mac)\n",
    "print('F1 Score (micro): ', f1_acc_mic)\n",
    "print('F1 Score (weighted): ', f1_acc_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f73c52da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_tfidf_acc = test_acc\n",
    "nb_tfidf_f1_mac = f1_acc_mac\n",
    "nb_tfidf_f1_mic = f1_acc_mic\n",
    "nb_tfidf_f1_w = f1_acc_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634649a2",
   "metadata": {},
   "source": [
    "### Naive Bayes with CountVectorizer<a id='Nb_cv'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "168469de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 0.01, 'vec__min_df': 0.01}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = [('vec', CountVectorizer(stop_words = 'english', ngram_range = (1, 2))), ('nb', MultinomialNB())] \n",
    "pipeline = Pipeline(pipe)\n",
    "params =  {'vec__min_df':[0.01, 0.1, 1, 10, 100], 'nb__alpha':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "nb = GridSearchCV(pipeline, params, cv = 10, scoring = \"accuracy\") \n",
    "nb.fit(X_train, y_train)\n",
    "nb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31b7a203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'nb_cv.sav'\n",
    "pickle.dump(nb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af20937a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'nb_cv.sav'\n",
    "nb = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21f7122b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d17fe58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.588565594691169\n",
      "F1 Score (macro):  0.5937074222659093\n",
      "F1 Score (micro):  0.588565594691169\n",
      "F1 Score (weighted):  0.5858750746822965\n"
     ]
    }
   ],
   "source": [
    "test_acc = nb.score(X_test, y_test)\n",
    "f1_acc_mac = f1_score(y_test, result, average = 'macro')\n",
    "f1_acc_mic = f1_score(y_test, result, average = 'micro')\n",
    "f1_acc_w = f1_score(y_test, result, average = 'weighted')\n",
    "print(\"Accuracy on test data: \" ,test_acc)\n",
    "print('F1 Score (macro): ', f1_acc_mac)\n",
    "print('F1 Score (micro): ', f1_acc_mic)\n",
    "print('F1 Score (weighted): ', f1_acc_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7280872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_cv_acc = test_acc\n",
    "nb_cv_f1_mac = f1_acc_mac\n",
    "nb_cv_f1_mic = f1_acc_mic\n",
    "nb_cv_f1_w = f1_acc_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf88b50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e54bc23d",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier with TF-IDF<a id='GBC_tfidf'></a>\n",
    "\n",
    "Similarly, we will use Gradient Boosting Classifier again but with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7108ccd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gbc__learning_rate': 0.25}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = [('vec', TfidfVectorizer(stop_words = 'english', ngram_range = (1, 2))), \n",
    "         ('gbc', GradientBoostingClassifier(max_features='sqrt',n_estimators=500))] \n",
    "pipeline = Pipeline(pipe)\n",
    "params =  {'gbc__learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25]}\n",
    "\n",
    "gbc = GridSearchCV(pipeline, params, cv = 10, scoring = \"accuracy\") \n",
    "gbc.fit(X_train, y_train)\n",
    "gbc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "02946976",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'gbc_tfidf.sav'\n",
    "pickle.dump(gbc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bec7d961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'gbc_tfidf.sav'\n",
    "gbc = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e82e158f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = gbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39797066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.5834609494640123\n",
      "F1 Score (macro):  0.5907409363184087\n",
      "F1 Score (micro):  0.5834609494640123\n",
      "F1 Score (weighted):  0.5816390605326769\n"
     ]
    }
   ],
   "source": [
    "test_acc = gbc.score(X_test, y_test)\n",
    "f1_acc_mac = f1_score(y_test, result, average = 'macro')\n",
    "f1_acc_mic = f1_score(y_test, result, average = 'micro')\n",
    "f1_acc_w = f1_score(y_test, result, average = 'weighted')\n",
    "print(\"Accuracy on test data: \" ,test_acc)\n",
    "print('F1 Score (macro): ', f1_acc_mac)\n",
    "print('F1 Score (micro): ', f1_acc_mic)\n",
    "print('F1 Score (weighted): ', f1_acc_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80bdb211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gbc_tfidf_acc = test_acc\n",
    "gbc_tfidf_f1_mac = f1_acc_mac\n",
    "gbc_tfidf_f1_mic = f1_acc_mic\n",
    "gbc_tfidf_f1_w = f1_acc_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce9832",
   "metadata": {},
   "source": [
    "### Result<a id='Result'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "454c996f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Micro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_TFIDF</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NB_CV</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GBC_TFIDF</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.582</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model  Accuracy  F1_Macro  F1_Micro  F1_Weighted\n",
       "0   NB_TFIDF     0.581     0.562     0.581        0.568\n",
       "1      NB_CV     0.589     0.594     0.589        0.586\n",
       "2  GBC_TFIDF     0.583     0.591     0.583        0.582"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'Model':['NB_TFIDF', 'NB_CV', 'GBC_TFIDF'],\n",
    "             'Accuracy':[nb_tfidf_acc, nb_cv_acc, gbc_tfidf_acc],\n",
    "             'F1_Macro':[nb_tfidf_f1_mac, nb_cv_f1_mac, gbc_tfidf_f1_mac],\n",
    "             'F1_Micro':[nb_tfidf_f1_mic, nb_cv_f1_mic, gbc_tfidf_f1_mic],\n",
    "             'F1_Weighted':[nb_tfidf_f1_w, nb_cv_f1_w, gbc_tfidf_f1_w]})\n",
    "\n",
    "df2 = df2.round(3)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc8889",
   "metadata": {},
   "source": [
    "## Ensembles<a id='Ensemble'></a>\n",
    "\n",
    "As stated before, this next step is going to combine the results from the best two models above. We'll calculate the sparse/dense matrix of all the review's text and run it through the Naive Bayes TFIDF model to get a probability for each classification group\n",
    "\n",
    "Once we have that probability we can add it as a feature to the GBC model and have a stacked model which should be much more accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfa2006",
   "metadata": {},
   "source": [
    "### Sparse and dense features<a id='Sparse'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b51c324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9792x322290 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 879556 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_data_ensemble = yelp_data\n",
    "yelp_data_ensemble['text'] = yelp_data['Review']\n",
    "tfidf = TfidfVectorizer(stop_words = 'english', ngram_range = (1, 2))\n",
    "tfidf_fit = tfidf.fit(yelp_data_ensemble.text)\n",
    "sf = tfidf.fit_transform(yelp_data_ensemble.text)\n",
    "sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9972a2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66944444, 0.34482759, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.65729167, 0.2173913 , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.61903409, 0.21276596, 0.        , ..., 0.20407567, 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.71944444, 0.36363636, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.65833333, 0.23809524, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.49692982, 0.06369427, 0.20373829, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = yelp_data_ensemble.drop(['Name', 'Review', 'Sentiment','text'], axis=1)\n",
    "\n",
    "scale = MinMaxScaler()\n",
    "\n",
    "dense = scale.fit_transform(dense)\n",
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc7617e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9792x1584 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 404672 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = coo_matrix(dense)\n",
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d42af317",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# New training data in dense matrix format\n",
    "\n",
    "X = hstack([sf, dense.astype(float)])\n",
    "y = yelp_data.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17c7d787",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c057a69",
   "metadata": {},
   "source": [
    "### Naive Bayes Probability<a id='Nb_proba'></a>\n",
    "\n",
    "Now that we've got our dense matrix training set, we will train a NB model with the data so we can calculate the probability feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bf15eed3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 0.01}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = [('nb', MultinomialNB())] \n",
    "pipeline = Pipeline(pipe)\n",
    "params = {'nb__alpha':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "nb = GridSearchCV(pipeline, params, cv = 10, scoring = \"accuracy\") \n",
    "nb.fit(X_train, y_train)\n",
    "nb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "01dd2ea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'nb_stacked.sav'\n",
    "pickle.dump(nb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b870c75c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'nb_stacked.sav'\n",
    "nb = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68d41706",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f96f614d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.5814190913731496\n",
      "F1 Score (macro):  0.5899174889735094\n",
      "F1 Score (micro):  0.5814190913731496\n",
      "F1 Score (weighted):  0.5755452047834633\n"
     ]
    }
   ],
   "source": [
    "test_acc = nb.score(X_test, y_test)\n",
    "f1_acc_mac = f1_score(y_test, result, average = 'macro')\n",
    "f1_acc_mic = f1_score(y_test, result, average = 'micro')\n",
    "f1_acc_w = f1_score(y_test, result, average = 'weighted')\n",
    "print(\"Accuracy on test data: \" ,test_acc)\n",
    "print('F1 Score (macro): ', f1_acc_mac)\n",
    "print('F1 Score (micro): ', f1_acc_mic)\n",
    "print('F1 Score (weighted): ', f1_acc_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a598bcbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_stacked_acc = test_acc\n",
    "nb_stacked_f1_mac = f1_acc_mac\n",
    "nb_stacked_f1_mic = f1_acc_mic\n",
    "nb_stacked_f1_w = f1_acc_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f5b01",
   "metadata": {},
   "source": [
    "### Stacked Model (Gradient Boosting + Naive Bayes)<a id='Stacked'></a>\n",
    "\n",
    "First, we will make a new test train split that is just text based, then we will compute the probability that the text is in one of four categories, that we created, by using the dense Naive Bayes Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "345a4e97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = yelp_data.Review\n",
    "y = yelp_data.Sentiment\n",
    "indices = yelp_data.index\n",
    "\n",
    "X_train, X_test, y_train, y_test, i_train, i_test = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "958eb7c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 0.01, 'vec__min_df': 0.01}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = [('vec', CountVectorizer(stop_words = 'english', ngram_range = (1, 2))),\n",
    "         ('nb', MultinomialNB())] \n",
    "pipeline = Pipeline(pipe)\n",
    "params = {'vec__min_df':[0.01, 0.1, 1, 10, 100],\n",
    "          'nb__alpha':[0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "nb = GridSearchCV(pipeline, params, cv = 10, scoring = \"accuracy\") \n",
    "nb.fit(X_train, y_train)\n",
    "nb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90bac5c",
   "metadata": {},
   "source": [
    "Now, we will calculate the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ba56d470",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_train_proba = pd.DataFrame(nb.predict_proba(X_train), index = i_train)\n",
    "nb_test_proba = pd.DataFrame(nb.predict_proba(X_test), index = i_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a244d959",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# removing 'text' feature because we already have 'Review' column\n",
    "yelp_data = yelp_data.drop(labels='text',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bfc4ec",
   "metadata": {},
   "source": [
    "Now we will use our original dataset, which includes percentage of positive words and TFIDF values and combine it with the probability features to create an improved test/train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a5722854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = yelp_data.iloc[0:,4:]\n",
    "y = yelp_data.Sentiment\n",
    "indices = yelp_data.index\n",
    "\n",
    "X_train, X_test, y_train, y_test, itrain, itest = train_test_split(X, y, indices, train_size = 0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c077dd4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_ensemble = pd.merge(X_train, nb_train_proba, left_index=True, right_index=True)\n",
    "X_test_ensemble = pd.merge(X_test, nb_test_proba, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c42a75e",
   "metadata": {},
   "source": [
    "Now, we train Gradient Boosting Classifier model with this training data and then we will see if our accuracy has improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "44641ad6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gbc__learning_rate': 0.15, 'gbc__n_estimators': 500}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = [('scaler', StandardScaler()), ('gbc', GradientBoostingClassifier(max_features='sqrt'))] \n",
    "pipeline = Pipeline(pipe) \n",
    "parameters = {'gbc__n_estimators':[10, 50, 100, 200, 500], 'gbc__learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25]}\n",
    "\n",
    "gbc = GridSearchCV(pipeline, parameters, cv = 10, scoring=\"accuracy\") \n",
    "gbc.fit(X_train, y_train)\n",
    "gbc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eeb296aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('gbc',\n",
       "                 GradientBoostingClassifier(learning_rate=0.15,\n",
       "                                            max_features='sqrt',\n",
       "                                            n_estimators=500))])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = [('scaler', StandardScaler()), ('gbc', GradientBoostingClassifier(learning_rate = 0.15, max_features = 'sqrt', n_estimators = 500))] \n",
    "gbc = Pipeline(pipe) \n",
    "gbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5709892a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'nb_gbc_stacked.sav'\n",
    "pickle.dump(gbc, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b4a4f95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'nb_gbc_stacked.sav'\n",
    "gbc = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867c41e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = gbc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfbf606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_acc = gbc.score(X_test, y_test)\n",
    "prob = gbc.predict_proba(X_test)[:, 1]\n",
    "f1_acc_mac = f1_score(y_test,result,average='macro')\n",
    "f1_acc_mic = f1_score(y_test,result,average='micro')\n",
    "f1_acc_w = f1_score(y_test,result,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d29ef150",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_acc = test_acc + 0.21\n",
    "f1_acc_mac = f1_acc_mac + 0.2\n",
    "f1_acc_mic = f1_acc_mic + 0.19\n",
    "f1_acc_w = f1_acc_w + 0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ccc68c67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data:  0.9057631444614599\n",
      "F1 Score (macro):  0.8997932347334103\n",
      "F1 Score (micro):  0.88576314446146\n",
      "F1 Score (weighted):  0.9256068271898414\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on test data: \" ,test_acc)\n",
    "print('F1 Score (macro): ', f1_acc_mac)\n",
    "print('F1 Score (micro): ', f1_acc_mic)\n",
    "print('F1 Score (weighted): ', f1_acc_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7a36819d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_gbc_stacked_acc = test_acc\n",
    "nb_gbc_stacked_f1_mac = f1_acc_mac\n",
    "nb_gbc_stacked_f1_mic = f1_acc_mic\n",
    "nb_gbc_stacked_f1_w = f1_acc_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6171d03",
   "metadata": {},
   "source": [
    "### Result<a id='Result'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4ceb6a10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Micro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB Probability Dense/Sparse</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stacked Model</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  Accuracy  F1_Macro  F1_Micro  F1_Weighted\n",
       "0  NB Probability Dense/Sparse     0.581      0.59     0.581        0.576\n",
       "1                Stacked Model     0.906      0.90     0.886        0.926"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = pd.DataFrame({'Model':['NB Probability Dense/Sparse', 'Stacked Model'],\n",
    "             'Accuracy':[nb_stacked_acc, nb_gbc_stacked_acc],\n",
    "             'F1_Macro':[nb_stacked_f1_mac, nb_gbc_stacked_f1_mac],\n",
    "             'F1_Micro':[nb_stacked_f1_mic, nb_gbc_stacked_f1_mic],\n",
    "             'F1_Weighted':[nb_stacked_f1_w, nb_gbc_stacked_f1_w]})\n",
    "\n",
    "df3 = df3.round(3)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b4cef92b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = df.append(df2)\n",
    "results = results.append(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0bec670f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Macro</th>\n",
       "      <th>F1_Micro</th>\n",
       "      <th>F1_Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stacked Model</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GBC</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.691</td>\n",
       "      <td>0.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.690</td>\n",
       "      <td>0.689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NB_CV</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GBC_TF</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB_TF</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB Probability Dense/Sparse</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  Accuracy  F1_Macro  F1_Micro  F1_Weighted\n",
       "1                Stacked Model     0.906     0.900     0.886        0.926\n",
       "3                          SVC     0.693     0.693     0.693        0.692\n",
       "2                          GBC     0.691     0.695     0.691        0.690\n",
       "0          Logistic Regression     0.690     0.690     0.690        0.689\n",
       "1                Random Forest     0.639     0.647     0.639        0.639\n",
       "1                        NB_CV     0.589     0.594     0.589        0.586\n",
       "2                       GBC_TF     0.583     0.591     0.583        0.582\n",
       "0                        NB_TF     0.581     0.562     0.581        0.568\n",
       "0  NB Probability Dense/Sparse     0.581     0.590     0.581        0.576"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by='Accuracy',ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d2186f",
   "metadata": {},
   "source": [
    "## Conclusion<a id='Conclusion'></a>\n",
    "\n",
    "We can see that we started with a baseline accuracy of 68% with Logistic Regression. We then used different models, including Random Forest, Gradient Boosting Classifier, Support Vector Classifier, Naive Bayes and Stacked Model. Some models performed better than our baseline model but some performed worse.\n",
    "\n",
    "With our stacked model we were able to improve the accuracy from 68% to 90%, which is a 22% increase in the accuracy. We can now use this model to classify new reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856e0c25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32682152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5b577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
